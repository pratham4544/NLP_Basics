{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b96cdc-9588-4d56-9a8f-ff8f56bd2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk -q\n",
    "!pip install spacy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34bd84ee-07d1-4b9b-bb29-b748a2446766",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''Hello Welcome, To Prathamesh's NLP Tutorials.\n",
    "Please do watch the entier couses! to become expert in NLP.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ebaaa6-57bf-41bf-9e9e-1bd85cde9353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, To Prathamesh's NLP Tutorials.\n",
      "Please do watch the entier couses! to become expert in NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fdf0da-1eda-4e98-9488-a58e8261218b",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e3563-1b5c-4799-8cec-8541c8dba430",
   "metadata": {},
   "source": [
    "### Sent Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b988744f-d5f3-4995-bf7a-a2e78830a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences --> paragraphs\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2255112-cd65-4f31-bef0-eeef996eb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8f75d0d-8b56-4099-a845-8aea30303d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hello Welcome, To Prathamesh's NLP Tutorials.\", 'Please do watch the entier couses!', 'to become expert in NLP.']\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5a564-50bd-498e-9193-0b4eaacc1943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef63fceb-167b-404f-85f6-8d11b44d9ebb",
   "metadata": {},
   "source": [
    "### Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62294592-3b9b-428d-bd77-0c5bc8e2dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragarphs --> words\n",
    "# sentence --> words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f478cad-9d0c-4dc6-a426-4b677d7e203c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'To',\n",
       " 'Prathamesh',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entier',\n",
       " 'couses',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "562d351f-327e-4a3e-9f43-c85cfc2ef54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'To', 'Prathamesh', \"'s\", 'NLP', 'Tutorials', '.']\n",
      "['Please', 'do', 'watch', 'the', 'entier', 'couses', '!']\n",
      "['to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "# documents --> words\n",
    "\n",
    "for i in documents:\n",
    "    print(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a390338-5c41-4568-a3bf-7984801d1b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6abf84fc-b140-4122-be1b-9dc886aa6cb3",
   "metadata": {},
   "source": [
    "### Word Punkt Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70b8336b-0dba-4868-a80d-b5fe2a880957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use punctions as sperate word\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e62f29ce-e962-44c5-ad7d-a6215d7265cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'To',\n",
       " 'Prathamesh',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entier',\n",
       " 'couses',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a9c26-61d8-4db8-a031-c2a285d0107c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d02b123-957d-4b85-87e4-091b1d59c2f0",
   "metadata": {},
   "source": [
    "### Tree Bank Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a613d26-ea80-42b7-ba70-ef90ebc860bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first full stop not be create a seperate words else last fullstop of document convert into seperate word\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40033405-9219-434e-993b-5acaecee90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d3e40fc-942b-445b-8ecf-55529baf63fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'To',\n",
       " 'Prathamesh',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entier',\n",
       " 'couses',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d79ee9-0991-4b84-b213-5b5ae448f265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
